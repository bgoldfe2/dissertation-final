{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19546,"status":"ok","timestamp":1678887161445,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":240},"id":"-gMWprA_zrAh","outputId":"f239b82a-0f1d-4244-b924-cbc9da31dd32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Github\n"]}],"source":["# This is a new start using the Bangledesh team's most recent version of their codebase\n","# Starting the port to Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/Github/"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oWyJXCRE089U","executionInfo":{"status":"ok","timestamp":1678887164619,"user_tz":240,"elapsed":3178,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"outputs":[],"source":["!git config --global user.email \"bgoldfe2@gmu.edu\"\n","!git config --global user.name \"Bruce Goldfeder\"\n","!cp -R ssh ~/.ssh"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678887164619,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":240},"id":"70JS5orH1ntq","outputId":"1d4d445a-7fb5-4aed-f1f4-d7059c28364c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/dissertation-final\n"]}],"source":["# %cd Extended-Cyberbullying-Detection/\n","%cd dissertation-final/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1678887164954,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":240},"id":"6uPJRiqh1rNj","outputId":"b187c5b3-b971-4d16-c917-0d9fa7261f68"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 47\n","drwx------ 2 root root 4096 Dec 27 18:26  Dataset\n","drwx------ 2 root root 4096 Dec 27 18:26  Figures\n","drwx------ 2 root root 4096 Jan 24 18:09  .git\n","-rw------- 1 root root   88 Mar  8 19:33  .gitignore\n","drwx------ 2 root root 4096 Dec 27 18:26  IPYNB\n","-rw------- 1 root root 1069 Jan 24 18:20  LICENSE\n","drwx------ 2 root root 4096 Dec 27 18:26  Models\n","drwx------ 2 root root 4096 Dec 27 18:26  Notebooks\n","drwx------ 2 root root 4096 Dec 27 18:26  Output\n","drwx------ 2 root root 4096 Dec 27 18:26 'Paper Figures'\n","-rw------- 1 root root  901 Feb 10 01:10  README.md\n","-rw------- 1 root root 7562 Feb 28 23:27  require_colab.txt\n","-rw------- 1 root root   45 Jan 24 18:20  requirements.txt\n","drwx------ 2 root root 4096 Dec 27 18:26  Scripts\n"]}],"source":["!ls -al"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12623,"status":"ok","timestamp":1678887177575,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":240},"id":"cKgN_RP71trx","outputId":"aae4e21f-ad20-4ef8-c2dd-b9f3d9d204ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.4.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (1.13.1+cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (1.22.4)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (3.9.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (23.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (2.25.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 2)) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.10)\n","Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","source":["!pip install deepspeed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nem8jg7jRDzK","executionInfo":{"status":"ok","timestamp":1678464296841,"user_tz":300,"elapsed":14889,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"8b52f9ae-e853-475c-d920-ad734f1b7771"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deepspeed\n","  Downloading deepspeed-0.8.2.tar.gz (763 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.5/763.5 KB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting hjson\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from deepspeed) (23.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from deepspeed) (5.4.8)\n","Collecting py-cpuinfo\n","  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.10.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.13.1+cu116)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from deepspeed) (4.65.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic->deepspeed) (4.5.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.8.2-py3-none-any.whl size=775202 sha256=c7b635a3a04c7668c7cdd9fe253ad5138868de3968f11d713429d566d9da5165\n","  Stored in directory: /root/.cache/pip/wheels/c7/f5/b5/aaddb0eadc99ea8e4c841bb7cac6ff5af7bc44cbdbebb042cb\n","Successfully built deepspeed\n","Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed\n","Successfully installed deepspeed-0.8.2 hjson-3.1.0 ninja-1.11.1 py-cpuinfo-9.0.0\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678887177575,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":240},"id":"OjK_XOMDf-Mk","outputId":"3ff1d1dc-0749-4319-e458-a6bdace3686e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/dissertation-final/Scripts\n"]}],"source":["%cd Scripts"]},{"cell_type":"code","source":["# Code block for running the evaluate and ensemble script\n","\n","#!python3 evaluate.py\n","\n","!python3 ensemble.py --ensemble_type asdf #max-voting"],"metadata":{"id":"I-xZr9hP5iTf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678887246642,"user_tz":240,"elapsed":69070,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"efca89fc-74f6-4e91-a0ae-0c0d4bc8f7d4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-15 13:33:06.767412: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-15 13:33:06.937905: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-15 13:33:07.687378: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-15 13:33:07.687482: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-15 13:33:07.687500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 85.9kB/s]\n","Downloading pytorch_model.bin: 100% 440M/440M [00:01<00:00, 242MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 760/760 [00:00<00:00, 294kB/s]\n","Downloading pytorch_model.bin: 100% 467M/467M [00:08<00:00, 54.1MB/s]\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 179kB/s]\n","Downloading pytorch_model.bin: 100% 501M/501M [00:01<00:00, 266MB/s]\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 684/684 [00:00<00:00, 259kB/s]\n","Downloading pytorch_model.bin: 100% 47.4M/47.4M [00:00<00:00, 67.1MB/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 1.01k/1.01k [00:00<00:00, 393kB/s]\n","Downloading pytorch_model.bin: 100% 526M/526M [00:07<00:00, 71.2MB/s]\n","Some weights of GPTNeoModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['transformer.h.9.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.11.attn.attention.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Github/dissertation-final/Scripts/ensemble.py\", line 270, in <module>\n","    averaging()\n","  File \"/content/drive/MyDrive/Github/dissertation-final/Scripts/ensemble.py\", line 141, in averaging\n","    bert, xlnet, roberta, albert, gpt2 = load_models()\n","  File \"/content/drive/MyDrive/Github/dissertation-final/Scripts/utils.py\", line 125, in load_models\n","    deberta.load_state_dict(torch.load(deberta_path))\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1671, in load_state_dict\n","    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n","RuntimeError: Error(s) in loading state_dict for DeBertaFGBC:\n","\tMissing key(s) in state_dict: \"DeBerta.embeddings.position_ids\", \"DeBerta.embeddings.word_embeddings.weight\", \"DeBerta.embeddings.position_embeddings.weight\", \"DeBerta.embeddings.token_type_embeddings.weight\", \"DeBerta.embeddings.LayerNorm.weight\", \"DeBerta.embeddings.LayerNorm.bias\", \"DeBerta.encoder.layer.0.attention.self.query.weight\", \"DeBerta.encoder.layer.0.attention.self.query.bias\", \"DeBerta.encoder.layer.0.attention.self.key.weight\", \"DeBerta.encoder.layer.0.attention.self.key.bias\", \"DeBerta.encoder.layer.0.attention.self.value.weight\", \"DeBerta.encoder.layer.0.attention.self.value.bias\", \"DeBerta.encoder.layer.0.attention.output.dense.weight\", \"DeBerta.encoder.layer.0.attention.output.dense.bias\", \"DeBerta.encoder.layer.0.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.0.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.0.intermediate.dense.weight\", \"DeBerta.encoder.layer.0.intermediate.dense.bias\", \"DeBerta.encoder.layer.0.output.dense.weight\", \"DeBerta.encoder.layer.0.output.dense.bias\", \"DeBerta.encoder.layer.0.output.LayerNorm.weight\", \"DeBerta.encoder.layer.0.output.LayerNorm.bias\", \"DeBerta.encoder.layer.1.attention.self.query.weight\", \"DeBerta.encoder.layer.1.attention.self.query.bias\", \"DeBerta.encoder.layer.1.attention.self.key.weight\", \"DeBerta.encoder.layer.1.attention.self.key.bias\", \"DeBerta.encoder.layer.1.attention.self.value.weight\", \"DeBerta.encoder.layer.1.attention.self.value.bias\", \"DeBerta.encoder.layer.1.attention.output.dense.weight\", \"DeBerta.encoder.layer.1.attention.output.dense.bias\", \"DeBerta.encoder.layer.1.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.1.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.1.intermediate.dense.weight\", \"DeBerta.encoder.layer.1.intermediate.dense.bias\", \"DeBerta.encoder.layer.1.output.dense.weight\", \"DeBerta.encoder.layer.1.output.dense.bias\", \"DeBerta.encoder.layer.1.output.LayerNorm.weight\", \"DeBerta.encoder.layer.1.output.LayerNorm.bias\", \"DeBerta.encoder.layer.2.attention.self.query.weight\", \"DeBerta.encoder.layer.2.attention.self.query.bias\", \"DeBerta.encoder.layer.2.attention.self.key.weight\", \"DeBerta.encoder.layer.2.attention.self.key.bias\", \"DeBerta.encoder.layer.2.attention.self.value.weight\", \"DeBerta.encoder.layer.2.attention.self.value.bias\", \"DeBerta.encoder.layer.2.attention.output.dense.weight\", \"DeBerta.encoder.layer.2.attention.output.dense.bias\", \"DeBerta.encoder.layer.2.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.2.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.2.intermediate.dense.weight\", \"DeBerta.encoder.layer.2.intermediate.dense.bias\", \"DeBerta.encoder.layer.2.output.dense.weight\", \"DeBerta.encoder.layer.2.output.dense.bias\", \"DeBerta.encoder.layer.2.output.LayerNorm.weight\", \"DeBerta.encoder.layer.2.output.LayerNorm.bias\", \"DeBerta.encoder.layer.3.attention.self.query.weight\", \"DeBerta.encoder.layer.3.attention.self.query.bias\", \"DeBerta.encoder.layer.3.attention.self.key.weight\", \"DeBerta.encoder.layer.3.attention.self.key.bias\", \"DeBerta.encoder.layer.3.attention.self.value.weight\", \"DeBerta.encoder.layer.3.attention.self.value.bias\", \"DeBerta.encoder.layer.3.attention.output.dense.weight\", \"DeBerta.encoder.layer.3.attention.output.dense.bias\", \"DeBerta.encoder.layer.3.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.3.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.3.intermediate.dense.weight\", \"DeBerta.encoder.layer.3.intermediate.dense.bias\", \"DeBerta.encoder.layer.3.output.dense.weight\", \"DeBerta.encoder.layer.3.output.dense.bias\", \"DeBerta.encoder.layer.3.output.LayerNorm.weight\", \"DeBerta.encoder.layer.3.output.LayerNorm.bias\", \"DeBerta.encoder.layer.4.attention.self.query.weight\", \"DeBerta.encoder.layer.4.attention.self.query.bias\", \"DeBerta.encoder.layer.4.attention.self.key.weight\", \"DeBerta.encoder.layer.4.attention.self.key.bias\", \"DeBerta.encoder.layer.4.attention.self.value.weight\", \"DeBerta.encoder.layer.4.attention.self.value.bias\", \"DeBerta.encoder.layer.4.attention.output.dense.weight\", \"DeBerta.encoder.layer.4.attention.output.dense.bias\", \"DeBerta.encoder.layer.4.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.4.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.4.intermediate.dense.weight\", \"DeBerta.encoder.layer.4.intermediate.dense.bias\", \"DeBerta.encoder.layer.4.output.dense.weight\", \"DeBerta.encoder.layer.4.output.dense.bias\", \"DeBerta.encoder.layer.4.output.LayerNorm.weight\", \"DeBerta.encoder.layer.4.output.LayerNorm.bias\", \"DeBerta.encoder.layer.5.attention.self.query.weight\", \"DeBerta.encoder.layer.5.attention.self.query.bias\", \"DeBerta.encoder.layer.5.attention.self.key.weight\", \"DeBerta.encoder.layer.5.attention.self.key.bias\", \"DeBerta.encoder.layer.5.attention.self.value.weight\", \"DeBerta.encoder.layer.5.attention.self.value.bias\", \"DeBerta.encoder.layer.5.attention.output.dense.weight\", \"DeBerta.encoder.layer.5.attention.output.dense.bias\", \"DeBerta.encoder.layer.5.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.5.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.5.intermediate.dense.weight\", \"DeBerta.encoder.layer.5.intermediate.dense.bias\", \"DeBerta.encoder.layer.5.output.dense.weight\", \"DeBerta.encoder.layer.5.output.dense.bias\", \"DeBerta.encoder.layer.5.output.LayerNorm.weight\", \"DeBerta.encoder.layer.5.output.LayerNorm.bias\", \"DeBerta.encoder.layer.6.attention.self.query.weight\", \"DeBerta.encoder.layer.6.attention.self.query.bias\", \"DeBerta.encoder.layer.6.attention.self.key.weight\", \"DeBerta.encoder.layer.6.attention.self.key.bias\", \"DeBerta.encoder.layer.6.attention.self.value.weight\", \"DeBerta.encoder.layer.6.attention.self.value.bias\", \"DeBerta.encoder.layer.6.attention.output.dense.weight\", \"DeBerta.encoder.layer.6.attention.output.dense.bias\", \"DeBerta.encoder.layer.6.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.6.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.6.intermediate.dense.weight\", \"DeBerta.encoder.layer.6.intermediate.dense.bias\", \"DeBerta.encoder.layer.6.output.dense.weight\", \"DeBerta.encoder.layer.6.output.dense.bias\", \"DeBerta.encoder.layer.6.output.LayerNorm.weight\", \"DeBerta.encoder.layer.6.output.LayerNorm.bias\", \"DeBerta.encoder.layer.7.attention.self.query.weight\", \"DeBerta.encoder.layer.7.attention.self.query.bias\", \"DeBerta.encoder.layer.7.attention.self.key.weight\", \"DeBerta.encoder.layer.7.attention.self.key.bias\", \"DeBerta.encoder.layer.7.attention.self.value.weight\", \"DeBerta.encoder.layer.7.attention.self.value.bias\", \"DeBerta.encoder.layer.7.attention.output.dense.weight\", \"DeBerta.encoder.layer.7.attention.output.dense.bias\", \"DeBerta.encoder.layer.7.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.7.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.7.intermediate.dense.weight\", \"DeBerta.encoder.layer.7.intermediate.dense.bias\", \"DeBerta.encoder.layer.7.output.dense.weight\", \"DeBerta.encoder.layer.7.output.dense.bias\", \"DeBerta.encoder.layer.7.output.LayerNorm.weight\", \"DeBerta.encoder.layer.7.output.LayerNorm.bias\", \"DeBerta.encoder.layer.8.attention.self.query.weight\", \"DeBerta.encoder.layer.8.attention.self.query.bias\", \"DeBerta.encoder.layer.8.attention.self.key.weight\", \"DeBerta.encoder.layer.8.attention.self.key.bias\", \"DeBerta.encoder.layer.8.attention.self.value.weight\", \"DeBerta.encoder.layer.8.attention.self.value.bias\", \"DeBerta.encoder.layer.8.attention.output.dense.weight\", \"DeBerta.encoder.layer.8.attention.output.dense.bias\", \"DeBerta.encoder.layer.8.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.8.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.8.intermediate.dense.weight\", \"DeBerta.encoder.layer.8.intermediate.dense.bias\", \"DeBerta.encoder.layer.8.output.dense.weight\", \"DeBerta.encoder.layer.8.output.dense.bias\", \"DeBerta.encoder.layer.8.output.LayerNorm.weight\", \"DeBerta.encoder.layer.8.output.LayerNorm.bias\", \"DeBerta.encoder.layer.9.attention.self.query.weight\", \"DeBerta.encoder.layer.9.attention.self.query.bias\", \"DeBerta.encoder.layer.9.attention.self.key.weight\", \"DeBerta.encoder.layer.9.attention.self.key.bias\", \"DeBerta.encoder.layer.9.attention.self.value.weight\", \"DeBerta.encoder.layer.9.attention.self.value.bias\", \"DeBerta.encoder.layer.9.attention.output.dense.weight\", \"DeBerta.encoder.layer.9.attention.output.dense.bias\", \"DeBerta.encoder.layer.9.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.9.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.9.intermediate.dense.weight\", \"DeBerta.encoder.layer.9.intermediate.dense.bias\", \"DeBerta.encoder.layer.9.output.dense.weight\", \"DeBerta.encoder.layer.9.output.dense.bias\", \"DeBerta.encoder.layer.9.output.LayerNorm.weight\", \"DeBerta.encoder.layer.9.output.LayerNorm.bias\", \"DeBerta.encoder.layer.10.attention.self.query.weight\", \"DeBerta.encoder.layer.10.attention.self.query.bias\", \"DeBerta.encoder.layer.10.attention.self.key.weight\", \"DeBerta.encoder.layer.10.attention.self.key.bias\", \"DeBerta.encoder.layer.10.attention.self.value.weight\", \"DeBerta.encoder.layer.10.attention.self.value.bias\", \"DeBerta.encoder.layer.10.attention.output.dense.weight\", \"DeBerta.encoder.layer.10.attention.output.dense.bias\", \"DeBerta.encoder.layer.10.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.10.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.10.intermediate.dense.weight\", \"DeBerta.encoder.layer.10.intermediate.dense.bias\", \"DeBerta.encoder.layer.10.output.dense.weight\", \"DeBerta.encoder.layer.10.output.dense.bias\", \"DeBerta.encoder.layer.10.output.LayerNorm.weight\", \"DeBerta.encoder.layer.10.output.LayerNorm.bias\", \"DeBerta.encoder.layer.11.attention.self.query.weight\", \"DeBerta.encoder.layer.11.attention.self.query.bias\", \"DeBerta.encoder.layer.11.attention.self.key.weight\", \"DeBerta.encoder.layer.11.attention.self.key.bias\", \"DeBerta.encoder.layer.11.attention.self.value.weight\", \"DeBerta.encoder.layer.11.attention.self.value.bias\", \"DeBerta.encoder.layer.11.attention.output.dense.weight\", \"DeBerta.encoder.layer.11.attention.output.dense.bias\", \"DeBerta.encoder.layer.11.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.11.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.11.intermediate.dense.weight\", \"DeBerta.encoder.layer.11.intermediate.dense.bias\", \"DeBerta.encoder.layer.11.output.dense.weight\", \"DeBerta.encoder.layer.11.output.dense.bias\", \"DeBerta.encoder.layer.11.output.LayerNorm.weight\", \"DeBerta.encoder.layer.11.output.LayerNorm.bias\", \"DeBerta.pooler.dense.weight\", \"DeBerta.pooler.dense.bias\". \n","\tUnexpected key(s) in state_dict: \"Bert.embeddings.position_ids\", \"Bert.embeddings.word_embeddings.weight\", \"Bert.embeddings.position_embeddings.weight\", \"Bert.embeddings.token_type_embeddings.weight\", \"Bert.embeddings.LayerNorm.weight\", \"Bert.embeddings.LayerNorm.bias\", \"Bert.encoder.layer.0.attention.self.query.weight\", \"Bert.encoder.layer.0.attention.self.query.bias\", \"Bert.encoder.layer.0.attention.self.key.weight\", \"Bert.encoder.layer.0.attention.self.key.bias\", \"Bert.encoder.layer.0.attention.self.value.weight\", \"Bert.encoder.layer.0.attention.self.value.bias\", \"Bert.encoder.layer.0.attention.output.dense.weight\", \"Bert.encoder.layer.0.attention.output.dense.bias\", \"Bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.0.intermediate.dense.weight\", \"Bert.encoder.layer.0.intermediate.dense.bias\", \"Bert.encoder.layer.0.output.dense.weight\", \"Bert.encoder.layer.0.output.dense.bias\", \"Bert.encoder.layer.0.output.LayerNorm.weight\", \"Bert.encoder.layer.0.output.LayerNorm.bias\", \"Bert.encoder.layer.1.attention.self.query.weight\", \"Bert.encoder.layer.1.attention.self.query.bias\", \"Bert.encoder.layer.1.attention.self.key.weight\", \"Bert.encoder.layer.1.attention.self.key.bias\", \"Bert.encoder.layer.1.attention.self.value.weight\", \"Bert.encoder.layer.1.attention.self.value.bias\", \"Bert.encoder.layer.1.attention.output.dense.weight\", \"Bert.encoder.layer.1.attention.output.dense.bias\", \"Bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.1.intermediate.dense.weight\", \"Bert.encoder.layer.1.intermediate.dense.bias\", \"Bert.encoder.layer.1.output.dense.weight\", \"Bert.encoder.layer.1.output.dense.bias\", \"Bert.encoder.layer.1.output.LayerNorm.weight\", \"Bert.encoder.layer.1.output.LayerNorm.bias\", \"Bert.encoder.layer.2.attention.self.query.weight\", \"Bert.encoder.layer.2.attention.self.query.bias\", \"Bert.encoder.layer.2.attention.self.key.weight\", \"Bert.encoder.layer.2.attention.self.key.bias\", \"Bert.encoder.layer.2.attention.self.value.weight\", \"Bert.encoder.layer.2.attention.self.value.bias\", \"Bert.encoder.layer.2.attention.output.dense.weight\", \"Bert.encoder.layer.2.attention.output.dense.bias\", \"Bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.2.intermediate.dense.weight\", \"Bert.encoder.layer.2.intermediate.dense.bias\", \"Bert.encoder.layer.2.output.dense.weight\", \"Bert.encoder.layer.2.output.dense.bias\", \"Bert.encoder.layer.2.output.LayerNorm.weight\", \"Bert.encoder.layer.2.output.LayerNorm.bias\", \"Bert.encoder.layer.3.attention.self.query.weight\", \"Bert.encoder.layer.3.attention.self.query.bias\", \"Bert.encoder.layer.3.attention.self.key.weight\", \"Bert.encoder.layer.3.attention.self.key.bias\", \"Bert.encoder.layer.3.attention.self.value.weight\", \"Bert.encoder.layer.3.attention.self.value.bias\", \"Bert.encoder.layer.3.attention.output.dense.weight\", \"Bert.encoder.layer.3.attention.output.dense.bias\", \"Bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.3.intermediate.dense.weight\", \"Bert.encoder.layer.3.intermediate.dense.bias\", \"Bert.encoder.layer.3.output.dense.weight\", \"Bert.encoder.layer.3.output.dense.bias\", \"Bert.encoder.layer.3.output.LayerNorm.weight\", \"Bert.encoder.layer.3.output.LayerNorm.bias\", \"Bert.encoder.layer.4.attention.self.query.weight\", \"Bert.encoder.layer.4.attention.self.query.bias\", \"Bert.encoder.layer.4.attention.self.key.weight\", \"Bert.encoder.layer.4.attention.self.key.bias\", \"Bert.encoder.layer.4.attention.self.value.weight\", \"Bert.encoder.layer.4.attention.self.value.bias\", \"Bert.encoder.layer.4.attention.output.dense.weight\", \"Bert.encoder.layer.4.attention.output.dense.bias\", \"Bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.4.intermediate.dense.weight\", \"Bert.encoder.layer.4.intermediate.dense.bias\", \"Bert.encoder.layer.4.output.dense.weight\", \"Bert.encoder.layer.4.output.dense.bias\", \"Bert.encoder.layer.4.output.LayerNorm.weight\", \"Bert.encoder.layer.4.output.LayerNorm.bias\", \"Bert.encoder.layer.5.attention.self.query.weight\", \"Bert.encoder.layer.5.attention.self.query.bias\", \"Bert.encoder.layer.5.attention.self.key.weight\", \"Bert.encoder.layer.5.attention.self.key.bias\", \"Bert.encoder.layer.5.attention.self.value.weight\", \"Bert.encoder.layer.5.attention.self.value.bias\", \"Bert.encoder.layer.5.attention.output.dense.weight\", \"Bert.encoder.layer.5.attention.output.dense.bias\", \"Bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.5.intermediate.dense.weight\", \"Bert.encoder.layer.5.intermediate.dense.bias\", \"Bert.encoder.layer.5.output.dense.weight\", \"Bert.encoder.layer.5.output.dense.bias\", \"Bert.encoder.layer.5.output.LayerNorm.weight\", \"Bert.encoder.layer.5.output.LayerNorm.bias\", \"Bert.encoder.layer.6.attention.self.query.weight\", \"Bert.encoder.layer.6.attention.self.query.bias\", \"Bert.encoder.layer.6.attention.self.key.weight\", \"Bert.encoder.layer.6.attention.self.key.bias\", \"Bert.encoder.layer.6.attention.self.value.weight\", \"Bert.encoder.layer.6.attention.self.value.bias\", \"Bert.encoder.layer.6.attention.output.dense.weight\", \"Bert.encoder.layer.6.attention.output.dense.bias\", \"Bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.6.intermediate.dense.weight\", \"Bert.encoder.layer.6.intermediate.dense.bias\", \"Bert.encoder.layer.6.output.dense.weight\", \"Bert.encoder.layer.6.output.dense.bias\", \"Bert.encoder.layer.6.output.LayerNorm.weight\", \"Bert.encoder.layer.6.output.LayerNorm.bias\", \"Bert.encoder.layer.7.attention.self.query.weight\", \"Bert.encoder.layer.7.attention.self.query.bias\", \"Bert.encoder.layer.7.attention.self.key.weight\", \"Bert.encoder.layer.7.attention.self.key.bias\", \"Bert.encoder.layer.7.attention.self.value.weight\", \"Bert.encoder.layer.7.attention.self.value.bias\", \"Bert.encoder.layer.7.attention.output.dense.weight\", \"Bert.encoder.layer.7.attention.output.dense.bias\", \"Bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.7.intermediate.dense.weight\", \"Bert.encoder.layer.7.intermediate.dense.bias\", \"Bert.encoder.layer.7.output.dense.weight\", \"Bert.encoder.layer.7.output.dense.bias\", \"Bert.encoder.layer.7.output.LayerNorm.weight\", \"Bert.encoder.layer.7.output.LayerNorm.bias\", \"Bert.encoder.layer.8.attention.self.query.weight\", \"Bert.encoder.layer.8.attention.self.query.bias\", \"Bert.encoder.layer.8.attention.self.key.weight\", \"Bert.encoder.layer.8.attention.self.key.bias\", \"Bert.encoder.layer.8.attention.self.value.weight\", \"Bert.encoder.layer.8.attention.self.value.bias\", \"Bert.encoder.layer.8.attention.output.dense.weight\", \"Bert.encoder.layer.8.attention.output.dense.bias\", \"Bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.8.intermediate.dense.weight\", \"Bert.encoder.layer.8.intermediate.dense.bias\", \"Bert.encoder.layer.8.output.dense.weight\", \"Bert.encoder.layer.8.output.dense.bias\", \"Bert.encoder.layer.8.output.LayerNorm.weight\", \"Bert.encoder.layer.8.output.LayerNorm.bias\", \"Bert.encoder.layer.9.attention.self.query.weight\", \"Bert.encoder.layer.9.attention.self.query.bias\", \"Bert.encoder.layer.9.attention.self.key.weight\", \"Bert.encoder.layer.9.attention.self.key.bias\", \"Bert.encoder.layer.9.attention.self.value.weight\", \"Bert.encoder.layer.9.attention.self.value.bias\", \"Bert.encoder.layer.9.attention.output.dense.weight\", \"Bert.encoder.layer.9.attention.output.dense.bias\", \"Bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.9.intermediate.dense.weight\", \"Bert.encoder.layer.9.intermediate.dense.bias\", \"Bert.encoder.layer.9.output.dense.weight\", \"Bert.encoder.layer.9.output.dense.bias\", \"Bert.encoder.layer.9.output.LayerNorm.weight\", \"Bert.encoder.layer.9.output.LayerNorm.bias\", \"Bert.encoder.layer.10.attention.self.query.weight\", \"Bert.encoder.layer.10.attention.self.query.bias\", \"Bert.encoder.layer.10.attention.self.key.weight\", \"Bert.encoder.layer.10.attention.self.key.bias\", \"Bert.encoder.layer.10.attention.self.value.weight\", \"Bert.encoder.layer.10.attention.self.value.bias\", \"Bert.encoder.layer.10.attention.output.dense.weight\", \"Bert.encoder.layer.10.attention.output.dense.bias\", \"Bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.10.intermediate.dense.weight\", \"Bert.encoder.layer.10.intermediate.dense.bias\", \"Bert.encoder.layer.10.output.dense.weight\", \"Bert.encoder.layer.10.output.dense.bias\", \"Bert.encoder.layer.10.output.LayerNorm.weight\", \"Bert.encoder.layer.10.output.LayerNorm.bias\", \"Bert.encoder.layer.11.attention.self.query.weight\", \"Bert.encoder.layer.11.attention.self.query.bias\", \"Bert.encoder.layer.11.attention.self.key.weight\", \"Bert.encoder.layer.11.attention.self.key.bias\", \"Bert.encoder.layer.11.attention.self.value.weight\", \"Bert.encoder.layer.11.attention.self.value.bias\", \"Bert.encoder.layer.11.attention.output.dense.weight\", \"Bert.encoder.layer.11.attention.output.dense.bias\", \"Bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.11.intermediate.dense.weight\", \"Bert.encoder.layer.11.intermediate.dense.bias\", \"Bert.encoder.layer.11.output.dense.weight\", \"Bert.encoder.layer.11.output.dense.bias\", \"Bert.encoder.layer.11.output.LayerNorm.weight\", \"Bert.encoder.layer.11.output.LayerNorm.bias\", \"Bert.pooler.dense.weight\", \"Bert.pooler.dense.bias\". \n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDl54UWl3Sn6","outputId":"635bc6fe-7f09-4630-badc-7446102fb764","executionInfo":{"status":"ok","timestamp":1678743298552,"user_tz":240,"elapsed":974025,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-13 21:18:47.400842: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-13 21:18:47.552512: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-13 21:18:48.312660: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-13 21:18:48.312748: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-13 21:18:48.312766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","{'Religion', 'Others', 'Gender', 'Notcb', 'Ethnicity', 'Age'}\n","train len - 28623, valid len - 9541, test len - 9541\n","Unnamed: 0\n","text\n","label\n","target\n","train example text --  If you call yourself a Christian yet you support this bill and the man pushing for it, you should be ashamed of yourself! Jesus Christ himself was radical during his time. And if he were here present today, he will be crucified again under this proposed law. #JUNKTERRORBILLNOW \n","with target --  Religion\n","Downloading (…)okenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 8.74kB/s]\n","Downloading (…)lve/main/config.json: 100% 579/579 [00:00<00:00, 98.0kB/s]\n","Downloading spm.model: 100% 2.46M/2.46M [00:00<00:00, 31.7MB/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","train_dataset object is of type --  <class 'dataset.DatasetDeberta'>\n","Print Encoded Token Byte tensor at location 1 --  tensor([     1,    369,   3606,  13942,  30267,    452,  20117,  17062,   3606,\n","         19443,   5343,  24284,    715,    724,   1246,  85215,   2102,   1632,\n","           261,   3606,  19432,   3354,  36253,  10688,   1580,  30267,    300,\n","         26072,  31065, 109775,   9455,  45008,  36408,  11226,   1445,    260,\n","          4263,   2931,   3433,  21872,  14483,  19967,  29752,    261,   3433,\n","          9588,   3354, 106648,  51567,  16909,   8332,   5343, 102376,   5697,\n","           260,   2669, 110994,  36396,  63870,    983,  40949,  50931,      2,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0])\n","The Decoded Token Text tensor is --  ['[CLS]', '▁If', 'you', 'call', 'yourself', 'a', 'Christian', 'yet', 'you', 'support', 'this', 'bill', 'and', 'the', 'man', 'pushing', 'for', 'it', ',', 'you', 'should', 'be', 'asha', 'med', 'of', 'yourself', '!', 'Jesus', 'Christ', 'himself', 'was', 'radical', 'during', 'his', 'time', '.', 'And', 'if', 'he', 'were', 'here', 'present', 'today', ',', 'he', 'will', 'be', 'cruci', 'fied', 'again', 'under', 'this', 'proposed', 'law', '.', '#', 'JUN', 'KT', 'ERROR', 'B', 'ILL', 'NOW', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","The model in the args is  microsoft/deberta-v3-base\n","Downloading pytorch_model.bin: 100% 371M/371M [00:01<00:00, 190MB/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Model Class:  <class 'model.DeBertaFGBC'> Num Params:  183881286\n","<generator object Module.named_parameters at 0x7f3f3406e7b0>\n","This is the type for the optimizer parameters -  <class 'list'>\n","This is the shape of the optimizer parameterss -  (2,)\n","---Starting Training---\n","Epoch 1/4\n","----------\n","100% 895/895 [03:28<00:00,  4.29it/s, loss=1.08]\n","Epoch 1 --- Training loss: 1.0818597502215614 Training accuracy: 0.6035\n","100% 299/299 [00:21<00:00, 13.78it/s]\n","Epoch 1 --- Validation loss: 0.5473483606525089 Validation accuracy: 0.8174\n","Epoch 1 val_acc 0.8174 best_acc 0.0\n","Epoch 2/4\n","----------\n","100% 895/895 [03:24<00:00,  4.38it/s, loss=0.536]\n","Epoch 2 --- Training loss: 0.5357114566105038 Training accuracy: 0.8293\n","100% 299/299 [00:20<00:00, 14.35it/s]\n","Epoch 2 --- Validation loss: 0.4597993565343295 Validation accuracy: 0.8455\n","Epoch 2 val_acc 0.8455 best_acc 0.8174\n","Epoch 3/4\n","----------\n","100% 895/895 [03:24<00:00,  4.38it/s, loss=0.432]\n","Epoch 3 --- Training loss: 0.43185477654694177 Training accuracy: 0.8662\n","100% 299/299 [00:20<00:00, 14.33it/s]\n","Epoch 3 --- Validation loss: 0.4196746960332162 Validation accuracy: 0.8567\n","Epoch 3 val_acc 0.8567 best_acc 0.8455\n","Epoch 4/4\n","----------\n","100% 895/895 [03:24<00:00,  4.38it/s, loss=0.371]\n","Epoch 4 --- Training loss: 0.3706689262023851 Training accuracy: 0.8892\n","100% 299/299 [00:20<00:00, 14.34it/s]\n","Epoch 4 --- Validation loss: 0.4162333740239159 Validation accuracy: 0.8556\n","\n","---History---\n","defaultdict(<class 'list'>, {'train_acc': [0.6035, 0.8293, 0.8662, 0.8892], 'train_loss': [1.0818597502215614, 0.5357114566105038, 0.43185477654694177, 0.3706689262023851], 'val_acc': [0.8174, 0.8455, 0.8567, 0.8556], 'val_loss': [0.5473483606525089, 0.4597993565343295, 0.4196746960332162, 0.4162333740239159]})\n","##################################### Testing ############################################\n","\n","Evaluating: ---microsoft/deberta-v3-base---\n","\n","100% 299/299 [00:21<00:00, 13.77it/s]\n","Output length --- 9541, Prediction length --- 9541\n","Accuracy: 0.86070642490305\n","Mcc Score: 0.8335978605031251\n","Precision: 0.8587474326312722\n","Recall: 0.86070642490305\n","F1_score: 0.8580264408537343\n","classification_report:                precision    recall  f1-score   support\n","\n","           0     0.9771    0.9784    0.9777      1571\n","           1     0.9678    0.9855    0.9766      1586\n","           2     0.8733    0.8934    0.8832      1613\n","           3     0.7077    0.5701    0.6315      1584\n","           4     0.6779    0.7621    0.7176      1585\n","           5     0.9483    0.9738    0.9609      1602\n","\n","    accuracy                         0.8607      9541\n","   macro avg     0.8587    0.8605    0.8579      9541\n","weighted avg     0.8587    0.8607    0.8580      9541\n","\n","[[1537    8    2   18    6    0]\n"," [   2 1563    1    3   13    4]\n"," [   1   11 1441   79   75    6]\n"," [  19   16  101  903  477   68]\n"," [  13   13   98  246 1208    7]\n"," [   1    4    7   27    3 1560]]\n","ROC-AUC Score: 0.984723977842152\n","##################################### Task End ############################################\n"]}],"source":["# Test run for regression testing\n","\n","!python3 train.py --epochs 4 --train_batch_size 32 --pretrained_model \"microsoft/deberta-v3-base\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlDjmLLxeoFE"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOJ4D/iWbR5D5mXP5zZqJcf"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}