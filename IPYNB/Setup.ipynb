{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23174,"status":"ok","timestamp":1679771111217,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":300},"id":"-gMWprA_zrAh","outputId":"6ad4d7b8-9dd6-406f-b5a2-443f84ef381d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Github\n"]}],"source":["# This is a new start using the Bangledesh team's most recent version of their codebase\n","# Starting the port to Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/Github/"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oWyJXCRE089U","executionInfo":{"status":"ok","timestamp":1679771115659,"user_tz":300,"elapsed":1258,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"outputs":[],"source":["!git config --global user.email \"bgoldfe2@gmu.edu\"\n","!git config --global user.name \"Bruce Goldfeder\"\n","!cp -R ssh ~/.ssh"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679771116438,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":300},"id":"70JS5orH1ntq","outputId":"0668322e-56d4-4c97-b5a0-7c4d768eac8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/dissertation-final\n"]}],"source":["# %cd Extended-Cyberbullying-Detection/\n","%cd dissertation-final/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":407,"status":"ok","timestamp":1679771120606,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":300},"id":"6uPJRiqh1rNj","outputId":"a6edd983-54b2-45d2-cd1e-0df93b3a374a"},"outputs":[{"output_type":"stream","name":"stdout","text":["total 47\n","drwx------ 2 root root 4096 Dec 27 18:26  Dataset\n","drwx------ 2 root root 4096 Dec 27 18:26  Figures\n","drwx------ 2 root root 4096 Jan 24 18:09  .git\n","-rw------- 1 root root   88 Mar  8 19:33  .gitignore\n","drwx------ 2 root root 4096 Dec 27 18:26  IPYNB\n","-rw------- 1 root root 1069 Jan 24 18:20  LICENSE\n","drwx------ 2 root root 4096 Dec 27 18:26  Models\n","drwx------ 2 root root 4096 Dec 27 18:26  Notebooks\n","drwx------ 2 root root 4096 Dec 27 18:26  Output\n","drwx------ 2 root root 4096 Dec 27 18:26 'Paper Figures'\n","-rw------- 1 root root  901 Feb 10 01:10  README.md\n","-rw------- 1 root root 7562 Feb 28 23:27  require_colab.txt\n","-rw------- 1 root root   45 Jan 24 18:20  requirements.txt\n","drwx------ 2 root root 4096 Dec 27 18:26  Scripts\n"]}],"source":["!ls -al"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10943,"status":"ok","timestamp":1679771136735,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":300},"id":"cKgN_RP71trx","outputId":"2231f9a2-0d08-4a2a-cf7e-f63a1300b266"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.4.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (1.13.1+cu116)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (1.22.4)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (3.10.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (4.65.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (23.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (2022.10.31)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 2)) (2022.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 2)) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.26.15)\n","Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.27.3\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","source":["!pip install deepspeed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nem8jg7jRDzK","executionInfo":{"status":"ok","timestamp":1678464296841,"user_tz":300,"elapsed":14889,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"8b52f9ae-e853-475c-d920-ad734f1b7771"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting deepspeed\n","  Downloading deepspeed-0.8.2.tar.gz (763 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.5/763.5 KB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting hjson\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from deepspeed) (23.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from deepspeed) (5.4.8)\n","Collecting py-cpuinfo\n","  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.10.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from deepspeed) (1.13.1+cu116)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from deepspeed) (4.65.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic->deepspeed) (4.5.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.8.2-py3-none-any.whl size=775202 sha256=c7b635a3a04c7668c7cdd9fe253ad5138868de3968f11d713429d566d9da5165\n","  Stored in directory: /root/.cache/pip/wheels/c7/f5/b5/aaddb0eadc99ea8e4c841bb7cac6ff5af7bc44cbdbebb042cb\n","Successfully built deepspeed\n","Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed\n","Successfully installed deepspeed-0.8.2 hjson-3.1.0 ninja-1.11.1 py-cpuinfo-9.0.0\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1679771141028,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"},"user_tz":300},"id":"OjK_XOMDf-Mk","outputId":"905bf7b8-8e64-4b5a-ada0-31780d60a5a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/dissertation-final/Scripts\n"]}],"source":["%cd Scripts"]},{"cell_type":"code","source":["# Code block for running the evaluate and ensemble script\n","\n","#!python3 evaluate.py\n","\n","!python3 ensemble.py --ensemble_type asdf #max-voting"],"metadata":{"id":"I-xZr9hP5iTf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678887246642,"user_tz":240,"elapsed":69070,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}},"outputId":"efca89fc-74f6-4e91-a0ae-0c0d4bc8f7d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-15 13:33:06.767412: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-15 13:33:06.937905: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-03-15 13:33:07.687378: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-15 13:33:07.687482: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-15 13:33:07.687500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 85.9kB/s]\n","Downloading pytorch_model.bin: 100% 440M/440M [00:01<00:00, 242MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 760/760 [00:00<00:00, 294kB/s]\n","Downloading pytorch_model.bin: 100% 467M/467M [00:08<00:00, 54.1MB/s]\n","Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n","- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 179kB/s]\n","Downloading pytorch_model.bin: 100% 501M/501M [00:01<00:00, 266MB/s]\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 684/684 [00:00<00:00, 259kB/s]\n","Downloading pytorch_model.bin: 100% 47.4M/47.4M [00:00<00:00, 67.1MB/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading (…)lve/main/config.json: 100% 1.01k/1.01k [00:00<00:00, 393kB/s]\n","Downloading pytorch_model.bin: 100% 526M/526M [00:07<00:00, 71.2MB/s]\n","Some weights of GPTNeoModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['transformer.h.9.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.11.attn.attention.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Github/dissertation-final/Scripts/ensemble.py\", line 270, in <module>\n","    averaging()\n","  File \"/content/drive/MyDrive/Github/dissertation-final/Scripts/ensemble.py\", line 141, in averaging\n","    bert, xlnet, roberta, albert, gpt2 = load_models()\n","  File \"/content/drive/MyDrive/Github/dissertation-final/Scripts/utils.py\", line 125, in load_models\n","    deberta.load_state_dict(torch.load(deberta_path))\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1671, in load_state_dict\n","    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n","RuntimeError: Error(s) in loading state_dict for DeBertaFGBC:\n","\tMissing key(s) in state_dict: \"DeBerta.embeddings.position_ids\", \"DeBerta.embeddings.word_embeddings.weight\", \"DeBerta.embeddings.position_embeddings.weight\", \"DeBerta.embeddings.token_type_embeddings.weight\", \"DeBerta.embeddings.LayerNorm.weight\", \"DeBerta.embeddings.LayerNorm.bias\", \"DeBerta.encoder.layer.0.attention.self.query.weight\", \"DeBerta.encoder.layer.0.attention.self.query.bias\", \"DeBerta.encoder.layer.0.attention.self.key.weight\", \"DeBerta.encoder.layer.0.attention.self.key.bias\", \"DeBerta.encoder.layer.0.attention.self.value.weight\", \"DeBerta.encoder.layer.0.attention.self.value.bias\", \"DeBerta.encoder.layer.0.attention.output.dense.weight\", \"DeBerta.encoder.layer.0.attention.output.dense.bias\", \"DeBerta.encoder.layer.0.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.0.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.0.intermediate.dense.weight\", \"DeBerta.encoder.layer.0.intermediate.dense.bias\", \"DeBerta.encoder.layer.0.output.dense.weight\", \"DeBerta.encoder.layer.0.output.dense.bias\", \"DeBerta.encoder.layer.0.output.LayerNorm.weight\", \"DeBerta.encoder.layer.0.output.LayerNorm.bias\", \"DeBerta.encoder.layer.1.attention.self.query.weight\", \"DeBerta.encoder.layer.1.attention.self.query.bias\", \"DeBerta.encoder.layer.1.attention.self.key.weight\", \"DeBerta.encoder.layer.1.attention.self.key.bias\", \"DeBerta.encoder.layer.1.attention.self.value.weight\", \"DeBerta.encoder.layer.1.attention.self.value.bias\", \"DeBerta.encoder.layer.1.attention.output.dense.weight\", \"DeBerta.encoder.layer.1.attention.output.dense.bias\", \"DeBerta.encoder.layer.1.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.1.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.1.intermediate.dense.weight\", \"DeBerta.encoder.layer.1.intermediate.dense.bias\", \"DeBerta.encoder.layer.1.output.dense.weight\", \"DeBerta.encoder.layer.1.output.dense.bias\", \"DeBerta.encoder.layer.1.output.LayerNorm.weight\", \"DeBerta.encoder.layer.1.output.LayerNorm.bias\", \"DeBerta.encoder.layer.2.attention.self.query.weight\", \"DeBerta.encoder.layer.2.attention.self.query.bias\", \"DeBerta.encoder.layer.2.attention.self.key.weight\", \"DeBerta.encoder.layer.2.attention.self.key.bias\", \"DeBerta.encoder.layer.2.attention.self.value.weight\", \"DeBerta.encoder.layer.2.attention.self.value.bias\", \"DeBerta.encoder.layer.2.attention.output.dense.weight\", \"DeBerta.encoder.layer.2.attention.output.dense.bias\", \"DeBerta.encoder.layer.2.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.2.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.2.intermediate.dense.weight\", \"DeBerta.encoder.layer.2.intermediate.dense.bias\", \"DeBerta.encoder.layer.2.output.dense.weight\", \"DeBerta.encoder.layer.2.output.dense.bias\", \"DeBerta.encoder.layer.2.output.LayerNorm.weight\", \"DeBerta.encoder.layer.2.output.LayerNorm.bias\", \"DeBerta.encoder.layer.3.attention.self.query.weight\", \"DeBerta.encoder.layer.3.attention.self.query.bias\", \"DeBerta.encoder.layer.3.attention.self.key.weight\", \"DeBerta.encoder.layer.3.attention.self.key.bias\", \"DeBerta.encoder.layer.3.attention.self.value.weight\", \"DeBerta.encoder.layer.3.attention.self.value.bias\", \"DeBerta.encoder.layer.3.attention.output.dense.weight\", \"DeBerta.encoder.layer.3.attention.output.dense.bias\", \"DeBerta.encoder.layer.3.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.3.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.3.intermediate.dense.weight\", \"DeBerta.encoder.layer.3.intermediate.dense.bias\", \"DeBerta.encoder.layer.3.output.dense.weight\", \"DeBerta.encoder.layer.3.output.dense.bias\", \"DeBerta.encoder.layer.3.output.LayerNorm.weight\", \"DeBerta.encoder.layer.3.output.LayerNorm.bias\", \"DeBerta.encoder.layer.4.attention.self.query.weight\", \"DeBerta.encoder.layer.4.attention.self.query.bias\", \"DeBerta.encoder.layer.4.attention.self.key.weight\", \"DeBerta.encoder.layer.4.attention.self.key.bias\", \"DeBerta.encoder.layer.4.attention.self.value.weight\", \"DeBerta.encoder.layer.4.attention.self.value.bias\", \"DeBerta.encoder.layer.4.attention.output.dense.weight\", \"DeBerta.encoder.layer.4.attention.output.dense.bias\", \"DeBerta.encoder.layer.4.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.4.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.4.intermediate.dense.weight\", \"DeBerta.encoder.layer.4.intermediate.dense.bias\", \"DeBerta.encoder.layer.4.output.dense.weight\", \"DeBerta.encoder.layer.4.output.dense.bias\", \"DeBerta.encoder.layer.4.output.LayerNorm.weight\", \"DeBerta.encoder.layer.4.output.LayerNorm.bias\", \"DeBerta.encoder.layer.5.attention.self.query.weight\", \"DeBerta.encoder.layer.5.attention.self.query.bias\", \"DeBerta.encoder.layer.5.attention.self.key.weight\", \"DeBerta.encoder.layer.5.attention.self.key.bias\", \"DeBerta.encoder.layer.5.attention.self.value.weight\", \"DeBerta.encoder.layer.5.attention.self.value.bias\", \"DeBerta.encoder.layer.5.attention.output.dense.weight\", \"DeBerta.encoder.layer.5.attention.output.dense.bias\", \"DeBerta.encoder.layer.5.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.5.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.5.intermediate.dense.weight\", \"DeBerta.encoder.layer.5.intermediate.dense.bias\", \"DeBerta.encoder.layer.5.output.dense.weight\", \"DeBerta.encoder.layer.5.output.dense.bias\", \"DeBerta.encoder.layer.5.output.LayerNorm.weight\", \"DeBerta.encoder.layer.5.output.LayerNorm.bias\", \"DeBerta.encoder.layer.6.attention.self.query.weight\", \"DeBerta.encoder.layer.6.attention.self.query.bias\", \"DeBerta.encoder.layer.6.attention.self.key.weight\", \"DeBerta.encoder.layer.6.attention.self.key.bias\", \"DeBerta.encoder.layer.6.attention.self.value.weight\", \"DeBerta.encoder.layer.6.attention.self.value.bias\", \"DeBerta.encoder.layer.6.attention.output.dense.weight\", \"DeBerta.encoder.layer.6.attention.output.dense.bias\", \"DeBerta.encoder.layer.6.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.6.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.6.intermediate.dense.weight\", \"DeBerta.encoder.layer.6.intermediate.dense.bias\", \"DeBerta.encoder.layer.6.output.dense.weight\", \"DeBerta.encoder.layer.6.output.dense.bias\", \"DeBerta.encoder.layer.6.output.LayerNorm.weight\", \"DeBerta.encoder.layer.6.output.LayerNorm.bias\", \"DeBerta.encoder.layer.7.attention.self.query.weight\", \"DeBerta.encoder.layer.7.attention.self.query.bias\", \"DeBerta.encoder.layer.7.attention.self.key.weight\", \"DeBerta.encoder.layer.7.attention.self.key.bias\", \"DeBerta.encoder.layer.7.attention.self.value.weight\", \"DeBerta.encoder.layer.7.attention.self.value.bias\", \"DeBerta.encoder.layer.7.attention.output.dense.weight\", \"DeBerta.encoder.layer.7.attention.output.dense.bias\", \"DeBerta.encoder.layer.7.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.7.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.7.intermediate.dense.weight\", \"DeBerta.encoder.layer.7.intermediate.dense.bias\", \"DeBerta.encoder.layer.7.output.dense.weight\", \"DeBerta.encoder.layer.7.output.dense.bias\", \"DeBerta.encoder.layer.7.output.LayerNorm.weight\", \"DeBerta.encoder.layer.7.output.LayerNorm.bias\", \"DeBerta.encoder.layer.8.attention.self.query.weight\", \"DeBerta.encoder.layer.8.attention.self.query.bias\", \"DeBerta.encoder.layer.8.attention.self.key.weight\", \"DeBerta.encoder.layer.8.attention.self.key.bias\", \"DeBerta.encoder.layer.8.attention.self.value.weight\", \"DeBerta.encoder.layer.8.attention.self.value.bias\", \"DeBerta.encoder.layer.8.attention.output.dense.weight\", \"DeBerta.encoder.layer.8.attention.output.dense.bias\", \"DeBerta.encoder.layer.8.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.8.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.8.intermediate.dense.weight\", \"DeBerta.encoder.layer.8.intermediate.dense.bias\", \"DeBerta.encoder.layer.8.output.dense.weight\", \"DeBerta.encoder.layer.8.output.dense.bias\", \"DeBerta.encoder.layer.8.output.LayerNorm.weight\", \"DeBerta.encoder.layer.8.output.LayerNorm.bias\", \"DeBerta.encoder.layer.9.attention.self.query.weight\", \"DeBerta.encoder.layer.9.attention.self.query.bias\", \"DeBerta.encoder.layer.9.attention.self.key.weight\", \"DeBerta.encoder.layer.9.attention.self.key.bias\", \"DeBerta.encoder.layer.9.attention.self.value.weight\", \"DeBerta.encoder.layer.9.attention.self.value.bias\", \"DeBerta.encoder.layer.9.attention.output.dense.weight\", \"DeBerta.encoder.layer.9.attention.output.dense.bias\", \"DeBerta.encoder.layer.9.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.9.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.9.intermediate.dense.weight\", \"DeBerta.encoder.layer.9.intermediate.dense.bias\", \"DeBerta.encoder.layer.9.output.dense.weight\", \"DeBerta.encoder.layer.9.output.dense.bias\", \"DeBerta.encoder.layer.9.output.LayerNorm.weight\", \"DeBerta.encoder.layer.9.output.LayerNorm.bias\", \"DeBerta.encoder.layer.10.attention.self.query.weight\", \"DeBerta.encoder.layer.10.attention.self.query.bias\", \"DeBerta.encoder.layer.10.attention.self.key.weight\", \"DeBerta.encoder.layer.10.attention.self.key.bias\", \"DeBerta.encoder.layer.10.attention.self.value.weight\", \"DeBerta.encoder.layer.10.attention.self.value.bias\", \"DeBerta.encoder.layer.10.attention.output.dense.weight\", \"DeBerta.encoder.layer.10.attention.output.dense.bias\", \"DeBerta.encoder.layer.10.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.10.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.10.intermediate.dense.weight\", \"DeBerta.encoder.layer.10.intermediate.dense.bias\", \"DeBerta.encoder.layer.10.output.dense.weight\", \"DeBerta.encoder.layer.10.output.dense.bias\", \"DeBerta.encoder.layer.10.output.LayerNorm.weight\", \"DeBerta.encoder.layer.10.output.LayerNorm.bias\", \"DeBerta.encoder.layer.11.attention.self.query.weight\", \"DeBerta.encoder.layer.11.attention.self.query.bias\", \"DeBerta.encoder.layer.11.attention.self.key.weight\", \"DeBerta.encoder.layer.11.attention.self.key.bias\", \"DeBerta.encoder.layer.11.attention.self.value.weight\", \"DeBerta.encoder.layer.11.attention.self.value.bias\", \"DeBerta.encoder.layer.11.attention.output.dense.weight\", \"DeBerta.encoder.layer.11.attention.output.dense.bias\", \"DeBerta.encoder.layer.11.attention.output.LayerNorm.weight\", \"DeBerta.encoder.layer.11.attention.output.LayerNorm.bias\", \"DeBerta.encoder.layer.11.intermediate.dense.weight\", \"DeBerta.encoder.layer.11.intermediate.dense.bias\", \"DeBerta.encoder.layer.11.output.dense.weight\", \"DeBerta.encoder.layer.11.output.dense.bias\", \"DeBerta.encoder.layer.11.output.LayerNorm.weight\", \"DeBerta.encoder.layer.11.output.LayerNorm.bias\", \"DeBerta.pooler.dense.weight\", \"DeBerta.pooler.dense.bias\". \n","\tUnexpected key(s) in state_dict: \"Bert.embeddings.position_ids\", \"Bert.embeddings.word_embeddings.weight\", \"Bert.embeddings.position_embeddings.weight\", \"Bert.embeddings.token_type_embeddings.weight\", \"Bert.embeddings.LayerNorm.weight\", \"Bert.embeddings.LayerNorm.bias\", \"Bert.encoder.layer.0.attention.self.query.weight\", \"Bert.encoder.layer.0.attention.self.query.bias\", \"Bert.encoder.layer.0.attention.self.key.weight\", \"Bert.encoder.layer.0.attention.self.key.bias\", \"Bert.encoder.layer.0.attention.self.value.weight\", \"Bert.encoder.layer.0.attention.self.value.bias\", \"Bert.encoder.layer.0.attention.output.dense.weight\", \"Bert.encoder.layer.0.attention.output.dense.bias\", \"Bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.0.intermediate.dense.weight\", \"Bert.encoder.layer.0.intermediate.dense.bias\", \"Bert.encoder.layer.0.output.dense.weight\", \"Bert.encoder.layer.0.output.dense.bias\", \"Bert.encoder.layer.0.output.LayerNorm.weight\", \"Bert.encoder.layer.0.output.LayerNorm.bias\", \"Bert.encoder.layer.1.attention.self.query.weight\", \"Bert.encoder.layer.1.attention.self.query.bias\", \"Bert.encoder.layer.1.attention.self.key.weight\", \"Bert.encoder.layer.1.attention.self.key.bias\", \"Bert.encoder.layer.1.attention.self.value.weight\", \"Bert.encoder.layer.1.attention.self.value.bias\", \"Bert.encoder.layer.1.attention.output.dense.weight\", \"Bert.encoder.layer.1.attention.output.dense.bias\", \"Bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.1.intermediate.dense.weight\", \"Bert.encoder.layer.1.intermediate.dense.bias\", \"Bert.encoder.layer.1.output.dense.weight\", \"Bert.encoder.layer.1.output.dense.bias\", \"Bert.encoder.layer.1.output.LayerNorm.weight\", \"Bert.encoder.layer.1.output.LayerNorm.bias\", \"Bert.encoder.layer.2.attention.self.query.weight\", \"Bert.encoder.layer.2.attention.self.query.bias\", \"Bert.encoder.layer.2.attention.self.key.weight\", \"Bert.encoder.layer.2.attention.self.key.bias\", \"Bert.encoder.layer.2.attention.self.value.weight\", \"Bert.encoder.layer.2.attention.self.value.bias\", \"Bert.encoder.layer.2.attention.output.dense.weight\", \"Bert.encoder.layer.2.attention.output.dense.bias\", \"Bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.2.intermediate.dense.weight\", \"Bert.encoder.layer.2.intermediate.dense.bias\", \"Bert.encoder.layer.2.output.dense.weight\", \"Bert.encoder.layer.2.output.dense.bias\", \"Bert.encoder.layer.2.output.LayerNorm.weight\", \"Bert.encoder.layer.2.output.LayerNorm.bias\", \"Bert.encoder.layer.3.attention.self.query.weight\", \"Bert.encoder.layer.3.attention.self.query.bias\", \"Bert.encoder.layer.3.attention.self.key.weight\", \"Bert.encoder.layer.3.attention.self.key.bias\", \"Bert.encoder.layer.3.attention.self.value.weight\", \"Bert.encoder.layer.3.attention.self.value.bias\", \"Bert.encoder.layer.3.attention.output.dense.weight\", \"Bert.encoder.layer.3.attention.output.dense.bias\", \"Bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.3.intermediate.dense.weight\", \"Bert.encoder.layer.3.intermediate.dense.bias\", \"Bert.encoder.layer.3.output.dense.weight\", \"Bert.encoder.layer.3.output.dense.bias\", \"Bert.encoder.layer.3.output.LayerNorm.weight\", \"Bert.encoder.layer.3.output.LayerNorm.bias\", \"Bert.encoder.layer.4.attention.self.query.weight\", \"Bert.encoder.layer.4.attention.self.query.bias\", \"Bert.encoder.layer.4.attention.self.key.weight\", \"Bert.encoder.layer.4.attention.self.key.bias\", \"Bert.encoder.layer.4.attention.self.value.weight\", \"Bert.encoder.layer.4.attention.self.value.bias\", \"Bert.encoder.layer.4.attention.output.dense.weight\", \"Bert.encoder.layer.4.attention.output.dense.bias\", \"Bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.4.intermediate.dense.weight\", \"Bert.encoder.layer.4.intermediate.dense.bias\", \"Bert.encoder.layer.4.output.dense.weight\", \"Bert.encoder.layer.4.output.dense.bias\", \"Bert.encoder.layer.4.output.LayerNorm.weight\", \"Bert.encoder.layer.4.output.LayerNorm.bias\", \"Bert.encoder.layer.5.attention.self.query.weight\", \"Bert.encoder.layer.5.attention.self.query.bias\", \"Bert.encoder.layer.5.attention.self.key.weight\", \"Bert.encoder.layer.5.attention.self.key.bias\", \"Bert.encoder.layer.5.attention.self.value.weight\", \"Bert.encoder.layer.5.attention.self.value.bias\", \"Bert.encoder.layer.5.attention.output.dense.weight\", \"Bert.encoder.layer.5.attention.output.dense.bias\", \"Bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.5.intermediate.dense.weight\", \"Bert.encoder.layer.5.intermediate.dense.bias\", \"Bert.encoder.layer.5.output.dense.weight\", \"Bert.encoder.layer.5.output.dense.bias\", \"Bert.encoder.layer.5.output.LayerNorm.weight\", \"Bert.encoder.layer.5.output.LayerNorm.bias\", \"Bert.encoder.layer.6.attention.self.query.weight\", \"Bert.encoder.layer.6.attention.self.query.bias\", \"Bert.encoder.layer.6.attention.self.key.weight\", \"Bert.encoder.layer.6.attention.self.key.bias\", \"Bert.encoder.layer.6.attention.self.value.weight\", \"Bert.encoder.layer.6.attention.self.value.bias\", \"Bert.encoder.layer.6.attention.output.dense.weight\", \"Bert.encoder.layer.6.attention.output.dense.bias\", \"Bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.6.intermediate.dense.weight\", \"Bert.encoder.layer.6.intermediate.dense.bias\", \"Bert.encoder.layer.6.output.dense.weight\", \"Bert.encoder.layer.6.output.dense.bias\", \"Bert.encoder.layer.6.output.LayerNorm.weight\", \"Bert.encoder.layer.6.output.LayerNorm.bias\", \"Bert.encoder.layer.7.attention.self.query.weight\", \"Bert.encoder.layer.7.attention.self.query.bias\", \"Bert.encoder.layer.7.attention.self.key.weight\", \"Bert.encoder.layer.7.attention.self.key.bias\", \"Bert.encoder.layer.7.attention.self.value.weight\", \"Bert.encoder.layer.7.attention.self.value.bias\", \"Bert.encoder.layer.7.attention.output.dense.weight\", \"Bert.encoder.layer.7.attention.output.dense.bias\", \"Bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.7.intermediate.dense.weight\", \"Bert.encoder.layer.7.intermediate.dense.bias\", \"Bert.encoder.layer.7.output.dense.weight\", \"Bert.encoder.layer.7.output.dense.bias\", \"Bert.encoder.layer.7.output.LayerNorm.weight\", \"Bert.encoder.layer.7.output.LayerNorm.bias\", \"Bert.encoder.layer.8.attention.self.query.weight\", \"Bert.encoder.layer.8.attention.self.query.bias\", \"Bert.encoder.layer.8.attention.self.key.weight\", \"Bert.encoder.layer.8.attention.self.key.bias\", \"Bert.encoder.layer.8.attention.self.value.weight\", \"Bert.encoder.layer.8.attention.self.value.bias\", \"Bert.encoder.layer.8.attention.output.dense.weight\", \"Bert.encoder.layer.8.attention.output.dense.bias\", \"Bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.8.intermediate.dense.weight\", \"Bert.encoder.layer.8.intermediate.dense.bias\", \"Bert.encoder.layer.8.output.dense.weight\", \"Bert.encoder.layer.8.output.dense.bias\", \"Bert.encoder.layer.8.output.LayerNorm.weight\", \"Bert.encoder.layer.8.output.LayerNorm.bias\", \"Bert.encoder.layer.9.attention.self.query.weight\", \"Bert.encoder.layer.9.attention.self.query.bias\", \"Bert.encoder.layer.9.attention.self.key.weight\", \"Bert.encoder.layer.9.attention.self.key.bias\", \"Bert.encoder.layer.9.attention.self.value.weight\", \"Bert.encoder.layer.9.attention.self.value.bias\", \"Bert.encoder.layer.9.attention.output.dense.weight\", \"Bert.encoder.layer.9.attention.output.dense.bias\", \"Bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.9.intermediate.dense.weight\", \"Bert.encoder.layer.9.intermediate.dense.bias\", \"Bert.encoder.layer.9.output.dense.weight\", \"Bert.encoder.layer.9.output.dense.bias\", \"Bert.encoder.layer.9.output.LayerNorm.weight\", \"Bert.encoder.layer.9.output.LayerNorm.bias\", \"Bert.encoder.layer.10.attention.self.query.weight\", \"Bert.encoder.layer.10.attention.self.query.bias\", \"Bert.encoder.layer.10.attention.self.key.weight\", \"Bert.encoder.layer.10.attention.self.key.bias\", \"Bert.encoder.layer.10.attention.self.value.weight\", \"Bert.encoder.layer.10.attention.self.value.bias\", \"Bert.encoder.layer.10.attention.output.dense.weight\", \"Bert.encoder.layer.10.attention.output.dense.bias\", \"Bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.10.intermediate.dense.weight\", \"Bert.encoder.layer.10.intermediate.dense.bias\", \"Bert.encoder.layer.10.output.dense.weight\", \"Bert.encoder.layer.10.output.dense.bias\", \"Bert.encoder.layer.10.output.LayerNorm.weight\", \"Bert.encoder.layer.10.output.LayerNorm.bias\", \"Bert.encoder.layer.11.attention.self.query.weight\", \"Bert.encoder.layer.11.attention.self.query.bias\", \"Bert.encoder.layer.11.attention.self.key.weight\", \"Bert.encoder.layer.11.attention.self.key.bias\", \"Bert.encoder.layer.11.attention.self.value.weight\", \"Bert.encoder.layer.11.attention.self.value.bias\", \"Bert.encoder.layer.11.attention.output.dense.weight\", \"Bert.encoder.layer.11.attention.output.dense.bias\", \"Bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"Bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"Bert.encoder.layer.11.intermediate.dense.weight\", \"Bert.encoder.layer.11.intermediate.dense.bias\", \"Bert.encoder.layer.11.output.dense.weight\", \"Bert.encoder.layer.11.output.dense.bias\", \"Bert.encoder.layer.11.output.LayerNorm.weight\", \"Bert.encoder.layer.11.output.LayerNorm.bias\", \"Bert.pooler.dense.weight\", \"Bert.pooler.dense.bias\". \n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDl54UWl3Sn6","outputId":"906f56af-8a9a-4f86-f24e-23158687cb2d","executionInfo":{"status":"ok","timestamp":1679772001403,"user_tz":300,"elapsed":830950,"user":{"displayName":"Bruce Goldfeder","userId":"01568302622654738051"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-03-25 19:06:13.694069: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-25 19:06:13.694178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-03-25 19:06:13.694196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","{'Religion', 'Ethnicity', 'Gender', 'Others', 'Age'}\n","train len - 23848, valid len - 7949, test len - 7950\n","Unnamed: 0\n","index\n","text\n","label\n","target\n","train example text --  theres a group of high school girls in this theatre yall im abt to get bullied So hard \n","with target --  Age\n","Downloading (…)okenizer_config.json: 100% 52.0/52.0 [00:00<00:00, 7.78kB/s]\n","Downloading (…)lve/main/config.json: 100% 579/579 [00:00<00:00, 94.0kB/s]\n","Downloading spm.model: 100% 2.46M/2.46M [00:00<00:00, 30.8MB/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","train_dataset object is of type --  <class 'dataset.DatasetDeberta'>\n","Print Encoded Token Byte tensor at location 1 --  tensor([    1,   262, 61938, 10914,  1580,  6213,  7960, 31028,   547,  5343,\n","        52516, 38615, 51517, 40763,   725,  7570, 30764, 19894,  5274,  9868,\n","            2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0])\n","The Decoded Token Text tensor is --  ['[CLS]', '▁the', 'resa', 'group', 'of', 'high', 'school', 'girls', 'in', 'this', 'theatre', 'yal', 'lima', 'bt', 'to', 'get', 'bull', 'ied', 'So', 'hard', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","The model in the args is  microsoft/deberta-v3-base\n","Downloading pytorch_model.bin: 100% 371M/371M [00:03<00:00, 102MB/s]\n","Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Model Class:  <class 'model.DeBertaFGBC'> Num Params:  183881221\n","<generator object Module.named_parameters at 0x7fd7fde253c0>\n","This is the type for the optimizer parameters -  <class 'list'>\n","This is the shape of the optimizer parameterss -  (2,)\n","---Starting Training---\n","Epoch 1/4\n","----------\n","100% 746/746 [02:56<00:00,  4.22it/s, loss=0.779]\n","Epoch 1 --- Training loss: 0.7781974511275982 Training accuracy: 0.7282\n","100% 249/249 [00:18<00:00, 13.42it/s]\n","Epoch 1 --- Validation loss: 0.30390083747934626 Validation accuracy: 0.924\n","Epoch 1 val_acc 0.924 best_acc 0.0\n","Epoch 2/4\n","----------\n","100% 746/746 [02:52<00:00,  4.32it/s, loss=0.265]\n","Epoch 2 --- Training loss: 0.2644195634142643 Training accuracy: 0.9383\n","100% 249/249 [00:17<00:00, 14.10it/s]\n","Epoch 2 --- Validation loss: 0.19867733702901377 Validation accuracy: 0.952\n","Epoch 2 val_acc 0.952 best_acc 0.924\n","Epoch 3/4\n","----------\n","100% 746/746 [02:52<00:00,  4.32it/s, loss=0.189]\n","Epoch 3 --- Training loss: 0.18886880748892756 Training accuracy: 0.9567\n","100% 249/249 [00:17<00:00, 14.10it/s]\n","Epoch 3 --- Validation loss: 0.18166429357954775 Validation accuracy: 0.9565\n","Epoch 3 val_acc 0.9565 best_acc 0.952\n","Epoch 4/4\n","----------\n","100% 746/746 [02:52<00:00,  4.32it/s, loss=0.15]\n","Epoch 4 --- Training loss: 0.15006690607693335 Training accuracy: 0.9689\n","100% 249/249 [00:17<00:00, 14.05it/s]\n","Epoch 4 --- Validation loss: 0.1782246766140662 Validation accuracy: 0.9583\n","Epoch 4 val_acc 0.9583 best_acc 0.9565\n","\n","---History---\n","defaultdict(<class 'list'>, {'train_acc': [0.7282, 0.9383, 0.9567, 0.9689], 'train_loss': [0.7781974511275982, 0.2644195634142643, 0.18886880748892756, 0.15006690607693335], 'val_acc': [0.924, 0.952, 0.9565, 0.9583], 'val_loss': [0.30390083747934626, 0.19867733702901377, 0.18166429357954775, 0.1782246766140662]})\n","##################################### Testing ############################################\n","\n","Evaluating: ---microsoft/deberta-v3-base---\n","\n","100% 249/249 [00:18<00:00, 13.44it/s]\n","Output length --- 7950, Prediction length --- 7950\n","Accuracy: 0.9519496855345913\n","Mcc Score: 0.9400133534597213\n","Precision: 0.9519738553694077\n","Recall: 0.9519496855345913\n","F1_score: 0.9518156288576725\n","classification_report:                precision    recall  f1-score   support\n","\n","           0     0.9842    0.9867    0.9855      1582\n","           1     0.9836    0.9873    0.9855      1580\n","           2     0.8978    0.9310    0.9141      1594\n","           3     0.9830    0.9824    0.9827      1592\n","           4     0.9120    0.8733    0.8922      1602\n","\n","    accuracy                         0.9519      7950\n","   macro avg     0.9521    0.9522    0.9520      7950\n","weighted avg     0.9520    0.9519    0.9518      7950\n","\n","[[1561    2    2    0   17]\n"," [   1 1560    5    4   10]\n"," [   4    1 1484   10   95]\n"," [   3    6    6 1564   13]\n"," [  17   17  156   13 1399]]\n","ROC-AUC Score: 0.9943846267948261\n","##################################### Task End ############################################\n"]}],"source":["# Test run for regression testing\n","\n","!python3 train.py --epochs 4 --train_batch_size 32 --pretrained_model \"microsoft/deberta-v3-base\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlDjmLLxeoFE"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOJ4D/iWbR5D5mXP5zZqJcf"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}